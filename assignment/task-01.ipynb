{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01: Introduction to Pytorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this task is to get us thinking not just about training models, but about our *training pipelines*.\n",
    "\n",
    "A neural network is a function, $f$, that accepts in data inputs, $\\boldsymbol{X}$, and weights, $\\boldsymbol{\\Theta}$ that produces labels $\\boldsymbol{\\hat{y}}$,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = f(\\Theta; \\boldsymbol{X}).\n",
    "$$\n",
    "\n",
    "Meanwhile, a neural network training process, is itself a function, $g$, which accepts as input a dataset $x$, and for supervised algorithms a set of targets $y$, along with a set of parameters $\\boldsymbol{\\Omega}$ which define how the process is performed, and produces as output the weights of a neural network, $\\boldsymbol{\\Theta}$,\n",
    "\n",
    "$$\n",
    "\\Theta = g(\\boldsymbol{\\Omega}; \\boldsymbol{X}, \\boldsymbol{y}).\n",
    "$$\n",
    "\n",
    "It is helpful to think of the training function, $g$, as a pipeline, composed of several training steps, which can include preprocessing, post processing, etc.\n",
    "\n",
    "$$\n",
    "g = g_N \\circ\\ \\cdots\\ \\circ g_1.\n",
    "$$\n",
    "\n",
    "For example, $g_1$ might be a preprocessing step, then $g_2$ might be a training step, and $g_3$ might be a pruning step in a basic pipeline where data $(\\boldsymbol{X}, \\boldsymbol{y})$ goes in and weights $\\boldsymbol{\\Theta}$ come out.\n",
    "\n",
    "We will learn to think of the training process this way by modifying some example code for a basic MNIST classification task. We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01 - Part 1\n",
    "\n",
    "Your first task is to:\n",
    "\n",
    "* Add layer definitions to the following neural network class\n",
    "* Define the forward pass\n",
    "\n",
    "You can find starting architectures online. It is important to know there is no known theory to identify a best architecture *before* starting the problem. Trial and error (by iterative training and testing) is the only way to prove or disprove the utility of an architecture.\n",
    "\n",
    "That said, recall some intuition about the way linear and nonlinear transforms work. We know we need a chain of both to have any hope of solving this problem. We also know that we need some depth, and cannot solve this problem by width alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        raise NotImplementedError(\"Not implemented!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Not implemented!\")\n",
    "        # Uncomment the return once implemented\n",
    "        # return output\n",
    "\n",
    "\n",
    "def run_training_epoch(\n",
    "    training_params, model, device, train_loader, optimizer, epoch\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % training_params.log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def predict(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code: Training Pipeline\n",
    "\n",
    "For this assignment, the training pipeline is defined for you. Notice the similarities to the mathematical description of a trainer we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"Training parameters for a simple neural network trainer.\"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    test_batch_size: int = 1000\n",
    "    epochs: int = 14\n",
    "    lr: float = 1.0\n",
    "    gamma: float = 0.7\n",
    "    normalizer_mean = 0.1307\n",
    "    normalizer_std = 0.3081\n",
    "    no_cuda: bool = True  # Enable or disable CUDA\n",
    "    no_mps: bool = True  # Enable or disable GPU on MacOS\n",
    "    dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    log_interval: int = 10\n",
    "    save_model: bool = True\n",
    "\n",
    "\n",
    "def configure_training_device(training_params):\n",
    "    use_cuda = not training_params.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not training_params.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(training_params.seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {\"batch_size\": training_params.batch_size}\n",
    "    test_kwargs = {\"batch_size\": training_params.test_batch_size}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    return device, train_kwargs, test_kwargs\n",
    "\n",
    "\n",
    "def build_preprocessing_transform(training_params):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (training_params.normalizer_mean,),\n",
    "                (training_params.normalizer_std,),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "def build_data_loaders(train_kwargs, test_kwargs, transform):\n",
    "    dataset1 = datasets.MNIST(\n",
    "        \"../data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(training_params, device, train_loader, test_loader):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "    for epoch in range(1, training_params.epochs + 1):\n",
    "        run_training_epoch(\n",
    "            training_params, model, device, train_loader, optimizer, epoch\n",
    "        )\n",
    "        predict(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        if training_params.save_model:\n",
    "            torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Execute a Training Pipeline\n",
    "\n",
    "With our training steps defined in modular fashion, we can easily define and execute a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m     train_loader, test_loader \u001b[38;5;241m=\u001b[39m build_data_loaders(\n\u001b[1;32m      8\u001b[0m         train_kwargs, test_kwargs, transform\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     train(training_params, device, train_loader, test_loader)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mexecute_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mexecute_training_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m transform \u001b[38;5;241m=\u001b[39m build_preprocessing_transform(training_params)\n\u001b[1;32m      7\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m build_data_loaders(\n\u001b[1;32m      8\u001b[0m     train_kwargs, test_kwargs, transform\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(training_params, device, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(training_params, device, train_loader, test_loader):\n\u001b[0;32m---> 69\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     70\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdadelta(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mtraining_params\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m     71\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39mtraining_params\u001b[38;5;241m.\u001b[39mgamma)\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mNet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Net, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot implemented!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "def execute_training_pipeline():\n",
    "    training_params = TrainingParameters(epochs=1, dry_run=True)\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(\n",
    "        training_params\n",
    "    )\n",
    "    transform = build_preprocessing_transform(training_params)\n",
    "    train_loader, test_loader = build_data_loaders(\n",
    "        train_kwargs, test_kwargs, transform\n",
    "    )\n",
    "    train(training_params, device, train_loader, test_loader)\n",
    "\n",
    "\n",
    "execute_training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01 - Part 2: Explore Width\n",
    "\n",
    "Using the example above, define a network with a single hidden layer.\n",
    "\n",
    "Modify the trainer to store the train and test errors in a numpy vector.\n",
    "\n",
    "Create a for loop over to iterate through a few different amounts of hidden neurons and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "(It should be hard to get good convergence here - this is part of the exercise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#Defining a network with a single hidden layer\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)  # Fully connected layer from input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)  # Output layer with 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU to the hidden layer\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)  # Log softmax for classification output\n",
    "        return x\n",
    "\n",
    "#Modify the trainer to store errors\n",
    "import numpy as np\n",
    "\n",
    "def run_training_epoch(training_params, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % training_params.log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "    return train_loss / len(train_loader.dataset)  # Return average training loss\n",
    "\n",
    "# Modify predict to return test loss and accuracy\n",
    "def predict(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * accuracy,\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy  # Return test loss and accuracy\n",
    "\n",
    "#Create a for loop to iterate over different hidden sizes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explore_width(hidden_sizes, training_params, device, train_loader, test_loader):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        print(f\"Training with {hidden_size} hidden neurons...\")\n",
    "        model = SimpleNet(hidden_size).to(device)\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "        min_test_error = float('inf')  # To track the minimum test error\n",
    "        \n",
    "        for epoch in range(1, training_params.epochs + 1):\n",
    "            train_loss = run_training_epoch(training_params, model, device, train_loader, optimizer, epoch)\n",
    "            test_loss, _ = predict(model, device, test_loader)\n",
    "            \n",
    "            # Add print statements here to check the loss at the end of each epoch\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Epoch {epoch}: Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "            # Track minimum test error\n",
    "            if test_loss < min_test_error:\n",
    "                min_test_error = test_loss\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "\n",
    "        train_errors.append(train_loss)\n",
    "        test_errors.append(min_test_error)  # Store the minimum test error\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure()\n",
    "    plt.plot(hidden_sizes, test_errors, label=\"Test Error\")\n",
    "    plt.xlabel(\"Number of Hidden Neurons\")\n",
    "    plt.ylabel(\"Minimum Test Error\")\n",
    "    plt.title(\"Test Error vs. Hidden Layer Size\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "hidden_sizes = [16, 32, 64, 128, 256, 512]\n",
    "training_params = TrainingParameters(epochs=2, dry_run=False)\n",
    "device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "transform = build_preprocessing_transform(training_params)\n",
    "train_loader, test_loader = build_data_loaders(train_kwargs, test_kwargs, transform)\n",
    "\n",
    "# Run the experiment\n",
    "explore_width(hidden_sizes, training_params, device, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 01 - Part 3: Explore Depth\n",
    "\n",
    "Now using the example above, define several networks with increasing numbers of hidden layers (either convolutional or fully connected).\n",
    "\n",
    "As above, create a for loop over to iterate through a few different depths and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "This example should converge much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#Define a class for networks with varying depths\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, hidden_size, depth):\n",
    "        super(DeepNet, self).__init__()\n",
    "        layers = []\n",
    "        input_size = 28 * 28  # MNIST input size\n",
    "        output_size = 10  # MNIST has 10 output classes\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers (vary depth)\n",
    "        for _ in range(depth - 1):  # depth - 1 because we already added one layer\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        return F.log_softmax(self.network(x), dim=1)\n",
    "\n",
    "#Modify the training function to explore depths\n",
    "def explore_depth(depths, hidden_size, training_params, device, train_loader, test_loader):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for depth in depths:\n",
    "        print(f\"Training with {depth} hidden layers...\")\n",
    "        model = DeepNet(hidden_size, depth).to(device)\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "        min_test_error = float('inf')  # To track the minimum test error\n",
    "        \n",
    "        for epoch in range(1, training_params.epochs + 1):\n",
    "            train_loss = run_training_epoch(training_params, model, device, train_loader, optimizer, epoch)\n",
    "            test_loss, _ = predict(model, device, test_loader)\n",
    "\n",
    "            # Add print statements here to check the loss at the end of each epoch\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Epoch {epoch}: Test Loss: {test_loss:.4f}\")\n",
    "            \n",
    "            # Track minimum test error\n",
    "            if test_loss < min_test_error:\n",
    "                min_test_error = test_loss\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "\n",
    "        train_errors.append(train_loss)\n",
    "        test_errors.append(min_test_error)  # Store the minimum test error\n",
    "    \n",
    "    return train_errors, test_errors\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define depths to test and hidden size\n",
    "depths = [1, 2, 3, 4, 5]  # Vary depth\n",
    "hidden_size = 128  # Use a fixed hidden size\n",
    "\n",
    "# Run the exploration\n",
    "train_errors, test_errors = explore_depth(depths, hidden_size, training_params, device, train_loader, test_loader)\n",
    "\n",
    "# Plot the minimum test error with respect to depth\n",
    "plt.plot(depths, test_errors, marker='o')\n",
    "plt.xlabel('Depth (number of hidden layers)')\n",
    "plt.ylabel('Minimum Test Error')\n",
    "plt.title('Test Error vs. Network Depth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
